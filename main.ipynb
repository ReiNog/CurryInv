{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Finance Playground\n",
    "\n",
    "### Notebook dedicated to play with the financial functions that will be useful to investment management projects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. CDI - Brazilian Interbank Deposit Rate - Using the functions that work with the CDI rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from traceback_with_variables import activate_in_ipython_by_import\n",
    "import pandas as pd\n",
    "import bacen as bc\n",
    "import ir_calc as ir\n",
    "import cdi\n",
    "\n",
    "# Defite the full path where the csv file with the CDI historical values is stored\n",
    "db_cdi_file = 'D:\\Investiments\\Databases\\Indexes\\CDI.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Update the CDI database with the most recent data - this must be done once a day to keep the CDI database up to date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdi.update_cdi_db(db_cdi_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we can play with the CDI rate</p>\n",
    "<p>First we load the CDI do a data frame</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cdi = cdi.load_cdi(db_cdi_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets calculate the final amount of a deposit indexed to 100% of the CDI<br>\n",
    "Note that when the percentage = 100%, it is not necessary to inform it to the function cdi.cdi_accum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_amount = 100000.00\n",
    "issue_date = pd.to_datetime('20220103')\n",
    "maturity_date = pd.to_datetime('20220801')\n",
    "\n",
    "maturity_amount = initial_amount * cdi.cdi_accum(df_cdi, issue_date, maturity_date)\n",
    "\n",
    "print(f'Final amount = ${maturity_amount:,.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets find the final amount of a deposit indexed to 90% of the CDI<br>\n",
    "We need to inform the percentage to the function cdi.cdi_accum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_amount = 100000.00\n",
    "percentage = 0.9\n",
    "issue_date = pd.to_datetime('20220103')\n",
    "maturity_date = pd.to_datetime('20220801')\n",
    "\n",
    "maturity_amount = initial_amount * cdi.cdi_accum(df_cdi, issue_date, maturity_date, percentage)\n",
    "\n",
    "print(f'Final amount = ${maturity_amount:,.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Selic - Brazilian monetary policy interest rate - Using the functions that work with the Selic rate\n",
    "\n",
    "##### Selic rate works the same way as the CDI rate, both are expressed as a percentage per annum, based on a two hundred fifty-two (252) business days year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from traceback_with_variables import activate_in_ipython_by_import\n",
    "import pandas as pd\n",
    "import bacen as bc\n",
    "import ir_calc as ir\n",
    "import selic\n",
    "\n",
    "# Defite the full path where the csv file with the CDI historical values is stored\n",
    "db_selic_file = 'D:\\Investiments\\Databases\\Indexes\\Selic.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Update the Selic database with the most recent data - this must be done onde a day to keep the CDI database up to date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selic.update_selic_db(db_selic_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we can play with the Selic rate</p>\n",
    "<p>First we load the Selic do a data frame</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selic = selic.load_selic(db_selic_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets calculate the final amount of a deposit indexed to 100% of the Selic<br>\n",
    "Note that when the percentage = 100%, it is not necessary to inform it to the function selic.selic_accum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_amount = 100000.00\n",
    "issue_date = pd.to_datetime('20220103')\n",
    "maturity_date = pd.to_datetime('20220801')\n",
    "\n",
    "maturity_amount = initial_amount * selic.selic_accum(df_selic, issue_date, maturity_date)\n",
    "\n",
    "print(f'Final amount = ${maturity_amount:,.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets find the final amount of a deposit indexed to 90% of the Selic<br>\n",
    "We need to inform the percentage to the function selic.selic_accum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_amount = 100000.00\n",
    "percentage = 0.9\n",
    "issue_date = pd.to_datetime('20220103')\n",
    "maturity_date = pd.to_datetime('20220801')\n",
    "\n",
    "maturity_amount = initial_amount * selic.selic_accum(df_selic, issue_date, maturity_date, percentage)\n",
    "\n",
    "print(f'Final amount = ${maturity_amount:,.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Holydays and business days\n",
    "\n",
    "##### Some functions to find next business day, calculate number of business days between two dates, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import br_workdays as brbd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the number of business days between two dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date1 = pd.to_datetime('2022-04-03')\n",
    "date2 = pd.to_datetime('2022-05-03')\n",
    "\n",
    "num_bdays = brbd.num_br_bdays(date1, date2)\n",
    "print(num_bdays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the next business day - the default number of business days to add is 1, thus it is not necessary to pass 1 to the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date3 = brbd.next_br_bday(date1)\n",
    "print (date3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the date that is n bussines days forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "date3 = brbd.next_br_bday(date1,n)\n",
    "print (date3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the previous business day - the default number of business days to subtract is 1, thus it is not necessary to pass 1 to the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date3 = brbd.prev_br_bday(date1)\n",
    "print (date3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the date that is n bussines days backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = -3\n",
    "date3 = brbd.prev_br_bday(date1,n)\n",
    "print (date3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asking if a date is a business day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date1 = pd.to_datetime('2022-09-06')  # True - it is a business day\n",
    "is_bday = brbd.is_br_bday(date1)   \n",
    "if is_bday:\n",
    "    print('{date1} is a business day in Brazil'.format(date1=date1.strftime('%d/%m/%Y')))\n",
    "else:\n",
    "    print('{date1} is not a business day in Brazil'.format(date1=date1.strftime('%d/%m/%Y')))\n",
    "    next_bday = brbd.next_br_bday(date1)\n",
    "    print('The first business day after {date1} is {date2}'.format(date1=date1.strftime('%d/%m/%Y'), date2=next_bday.strftime('%d/%m/%Y')))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date1 = pd.to_datetime('2022-07-30') # False - it is a Saturday\n",
    "is_bday = brbd.is_br_bday(date1)   \n",
    "if is_bday:\n",
    "    print('{date1} is a business day in Brazil'.format(date1=date1.strftime('%d/%m/%Y')))\n",
    "else:\n",
    "    print('{date1} is not a business day in Brazil'.format(date1=date1.strftime('%d/%m/%Y')))\n",
    "    next_bday = brbd.next_br_bday(date1)\n",
    "    print('The first business day after {date1} is {date2}'.format(date1=date1.strftime('%d/%m/%Y'), date2=next_bday.strftime('%d/%m/%Y')))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date1 = pd.to_datetime('2022-09-07')   # False - it is Brazil's Independence Day\n",
    "is_bday = brbd.is_br_bday(date1)   \n",
    "if is_bday:\n",
    "    print('{date1} is a business day in Brazil'.format(date1=date1.strftime('%d/%m/%Y')))\n",
    "else:\n",
    "    print('{date1} is not a business day in Brazil'.format(date1=date1.strftime('%d/%m/%Y')))\n",
    "    next_bday = brbd.next_br_bday(date1)\n",
    "    print('The first business day after {date1} is {date2}'.format(date1=date1.strftime('%d/%m/%Y'), date2=next_bday.strftime('%d/%m/%Y')))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date1 = pd.to_datetime('2020-03-02') \n",
    "is_bday = brbd.is_br_bday(date1)   \n",
    "if is_bday:\n",
    "    print('{date1} is a business day in Brazil'.format(date1=date1.strftime('%d/%m/%Y')))\n",
    "else:\n",
    "    print('{date1} is not a business day in Brazil'.format(date1=date1.strftime('%d/%m/%Y')))\n",
    "    next_bday = brbd.next_br_bday(date1)\n",
    "    print('The first business day after {date1} is {date2}'.format(date1=date1.strftime('%d/%m/%Y'), date2=next_bday.strftime('%d/%m/%Y')))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Non-exaustive unit tests of the num_br_bdays function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dates = pd.DataFrame({'st_date':['20200201','20200101','20200203','20200203','20200203','20200203','20200203','20200203','20200217','20200217','20200217','20200217','20200217','20220401','20220501','20220403','20220501','20220415','20220501','20220416','20220510','20220425','20220501','20220416','20220410','20220425','20220403'], \n",
    "                        'end_date': ['20200301','20200201','20200303','20200301','20200215','20200216','20200210','20200225','20200301','20200316','20200310','20200225','20200303','20220501','20220601','20220503','20220503','20220503','20220516','20220516','20220516','20220516','20220503','20220503','20220503','20220503','20220503'], \n",
    "                        'num_bdays': [18,22,19,18,10,10,5,15,8,18,14,5,9,19,22,19,1,10,10,19,4,15,1,10,14,6,19]})\n",
    "test_dates['st_date'] = pd.to_datetime(test_dates['st_date'])\n",
    "test_dates['end_date'] = pd.to_datetime(test_dates['end_date'])\n",
    "error_count = 0\n",
    "\n",
    "for i in test_dates.index:\n",
    "    num_bdays = brbd.num_br_bdays(test_dates.loc[i]['st_date'], test_dates.loc[i]['end_date'])\n",
    "    if abs(num_bdays - test_dates.loc[i]['num_bdays']) > 0:\n",
    "        error_count += 1\n",
    "        print('Num bis days calculated differs from expected. Num bdays calculated: {bdays1}, Expected: {bdays2}'.format(bdays1=num_bdays, bdays2=test_dates.loc[i]['num_bdays']))\n",
    "\n",
    "print('Num errors = {nerr}'.format(nerr=error_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. IPCA - Brazilian official inflation index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ipca\n",
    "import br_workdays as wd\n",
    "import ibge\n",
    "\n",
    "path_ipca = 'D:\\Investiments\\Databases\\Indexes\\IPCA.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Downloading the IPCA rates from the IBGE API, updating the cumulative return and loading the IPCA database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ipca.update_ipca_db(path_ipca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ipca = ipca.load_ipca(path_ipca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating values indexed to IPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date1 = pd.to_datetime('20220422')\n",
    "date2 = pd.to_datetime('20220601')\n",
    "ini_amount = 39208600.79\n",
    "end_amount = ini_amount * ipca.ipca_accum(df_ipca, date1, date2, 0, 'cd')\n",
    "print('End amount = {:,.2f}'.format(end_amount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Non-exaustive unit tests of the ipca_accum function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-exaustive unit tests for the ipca_accum function - using calendar days for the pro-rata\n",
    "\n",
    "test_values_cd = pd.DataFrame({'issue_date': ['20200203','20200203','20200203','20200203','20200203','20200203','20200203','20200217','20200217','20200217','20200217','20200217'], \n",
    "                            'maturity_date': ['20220503','20220503','20220503','20220516','20220516','20220516','20220516','20220503','20220503','20220503','20220503','20220503'],\n",
    "                            'reset_day': [3,1,15,1,16,10,25,1,16,10,25,3],\n",
    "                            'f_accum': [1.19721652,1.19737254,1.19314617,1.19972932,1.19827020,1.19887131,1.19521345,1.19593011,1.19165612,1.19355504,1.18863815,1.19577427]})\n",
    "test_values_cd['issue_date'] = pd.to_datetime(test_values_cd['issue_date'])\n",
    "test_values_cd['maturity_date'] = pd.to_datetime(test_values_cd['maturity_date'])\n",
    "error_count = 0\n",
    "\n",
    "for x in test_values_cd.index:\n",
    "    f_accum_cd = ipca.ipca_accum(df_ipca, test_values_cd.loc[x]['issue_date'], test_values_cd.loc[x]['maturity_date'], test_values_cd.loc[x]['reset_day'], 'cd')\n",
    "    if abs(f_accum_cd - test_values_cd.loc[x]['f_accum']) > 0.00000002:\n",
    "        print('IPCA acummulated, calendar days, Start Date: {st_date}, End Date: {end_date}, Correct Accum = {ipca1:,.8f}, Calc Accum = {ipca2:,.8f}'.format(st_date=test_values_cd.loc[x]['issue_date'].strftime('%d/%m/%Y'),end_date=test_values_cd.loc[x]['maturity_date'].strftime('%d/%m/%Y'),ipca1=test_values_cd.loc[x]['f_accum'],ipca2=f_accum_cd))\n",
    "        error_count += 1\n",
    "\n",
    "print('Number of errors = {0}'.format(error_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-exaustive unit tests for the ipca_accum function - using business days for the pro-rata\n",
    "\n",
    "test_values_bd = pd.DataFrame({'issue_date': ['20200203','20200203','20200203','20200203','20200203','20200203','20200203','20200217','20200217','20200217','20200217','20200217'], \n",
    "                            'maturity_date': ['20220503','20220503','20220503','20220516','20220516','20220516','20220516','20220503','20220503','20220503','20220503','20220503'],\n",
    "                            'reset_day': [3,1,15,1,16,10,25,1,16,10,25,3],\n",
    "                            'f_accum': [1.19738260,1.19747171,1.19238823,1.19977094,1.19835866,1.19880905,1.19627167,1.19581179,1.19125177,1.19323683,1.18917716,1.19572280]})\n",
    "test_values_bd['issue_date'] = pd.to_datetime(test_values_bd['issue_date'])\n",
    "test_values_bd['maturity_date'] = pd.to_datetime(test_values_bd['maturity_date'])\n",
    "error_count = 0\n",
    "\n",
    "for x in test_values_bd.index:\n",
    "    f_accum_bd = ipca.ipca_accum(df_ipca, test_values_bd.loc[x]['issue_date'], test_values_bd.loc[x]['maturity_date'], test_values_bd.loc[x]['reset_day'], 'bd')\n",
    "    if abs(f_accum_bd - test_values_bd.loc[x]['f_accum']) > 0.00000002:\n",
    "        print('IPCA acummulated, calendar days, Start Date: {st_date}, End Date: {end_date}, Correct Accum = {ipca1:,.8f}, Calc Accum = {ipca2:,.8f}'.format(st_date=test_values_bd.loc[x]['issue_date'].strftime('%d/%m/%Y'),end_date=test_values_bd.loc[x]['maturity_date'].strftime('%d/%m/%Y'),ipca1=test_values_bd.loc[x]['f_accum'],ipca2=f_accum_bd))\n",
    "        error_count += 1\n",
    "\n",
    "print('Number of errors = {0}'.format(error_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. BRL/USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from traceback_with_variables import activate_in_ipython_by_import\n",
    "import pandas as pd\n",
    "import bacen as bc\n",
    "import br_workdays as wd\n",
    "import fxrates as fx\n",
    "\n",
    "db_path='D:\\Investiments\\Databases\\Indexes\\BRLUSD.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx.update_brlusd_db(db_path)\n",
    "df_brlusd = fx.load_brlusd(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brlusd.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date1 = pd.to_datetime('20220103')\n",
    "date2 = pd.to_datetime('20220803')\n",
    "ini_amount = 1000000.00\n",
    "end_amount = ini_amount * fx.brlusd_accum(df_brlusd, date1, date2)\n",
    "print('End amount = {:,}'.format(end_amount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping Unnamed columns\n",
    "#df_ = df_.loc[:,~df_.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# Changing the name of a column\n",
    "#df_.rename(columns={'Fund':'Asset'}, inplace=True)\n",
    "\n",
    "# Reading a csv file\n",
    "#df_trades_funds = pd.read_csv(db_path, delimiter=';', dtype={'Client': int, 'Book': str}, index_col='TradeDate')\n",
    "\n",
    "# Writing a csv file\n",
    "#df_.to_csv(db_path, sep=';',header=['Rate','Accum'], index_label='TradeDate')\n",
    "# trades_funds.to_csv(db_path, sep=';',header=['Client', 'Book', 'Strategy', 'Asset', 'BuySell', 'Quantity', 'Price', 'Fees', 'Taxes', 'GrossAmount', 'NetAmount', 'NetPrice', 'SettlDate', 'BuyDate'], index_label='TradeDate')\n",
    "\n",
    "# Drop RiskClass, AssetClass and Currency from Trades' files - those classifications will be in the Asset master registry\n",
    "#db_path = 'D:\\Investiments\\Databases\\Portfolios\\client-000001\\Trades-Funds.csv'\n",
    "#trades_funds = pd.read_csv(db_path, delimiter=';', dtype={'Client':int, 'Book':str, 'Strategy':str, 'RiskClass':str, 'AssetClass':str, 'Asset':str, 'Currency':str, 'BuySell':str, 'Quantity':float, 'Price':float, 'Fees':float, 'Taxes':float, \n",
    "#                                                            'GrossAmount':float, 'NetAmount':float, 'NetPrice':float, 'SettlDate':str, 'BuyDate':str}, index_col=['TradeDate'])\n",
    "#trades_funds.index = pd.to_datetime(trades_funds.index,format='%Y-%m-%d')\n",
    "#trades_funds['SettlDate'] = pd.to_datetime(trades_funds['SettlDate'],format='%Y-%m-%d')\n",
    "#trades_funds['BuyDate'] = pd.to_datetime(trades_funds['BuyDate'],format='%Y-%m-%d')\n",
    "#trades_funds.sort_index()\n",
    "#trades_funds.drop(['RiskClass', 'AssetClass', 'Currency'], axis=1, inplace=True)\n",
    "#trades_funds.to_csv(db_path, sep=';',header=['Client', 'Book', 'Strategy', 'Asset','BuySell', 'Quantity', 'Price', 'Fees', 'Taxes', 'GrossAmount', 'NetAmount', 'NetPrice', 'SettlDate', 'BuyDate'], index_label='TradeDate')\n",
    "\n",
    "# Medir tempo de execução\n",
    "#import timeit\n",
    "#start_time = timeit.default_timer()\n",
    "#end_time = timeit.default_timer()\n",
    "#print(end_time - start_time)\n",
    "\n",
    "#idx = pd.IndexSlice\n",
    "#portfolio.loc[idx[1,'Liquids_BR','Strategic','BTGP Yield DI']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import br_workdays as wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trades(db_path, start_date = pd.to_datetime('1900-01-01'), end_date = pd.to_datetime('2100-12-31')):\n",
    "    # Reads the specified trades database to dataframe and selects only the trades with start_date <= TradeDate <= end_date. If no dates are passed, gets all the trades available\n",
    "    error_msg = ''\n",
    "    try:\n",
    "        trades = pd.read_csv(db_path, delimiter=';', \n",
    "                              dtype={'Client': int, 'Book': str, 'Strategy': str, 'Asset': str, 'Tr_BuySell':str, 'Tr_Quantity':float, 'Tr_Price': float, 'Tr_Fees': float, 'Tr_Taxes':float, 'Tr_GrossAmount': float, 'Tr_NetAmount': float, 'Tr_NetPrice': float, 'Tr_SettlDate': str, 'Tr_BuyDate': str}, \n",
    "                              index_col=['TradeDate'])\n",
    "        trades.index = pd.to_datetime(trades.index,format='%Y-%m-%d')\n",
    "        trades['Tr_SettlDate'] = pd.to_datetime(trades['Tr_SettlDate'],format='%Y-%m-%d')\n",
    "        trades['Tr_BuyDate'] = pd.to_datetime(trades['Tr_BuyDate'],format='%Y-%m-%d')\n",
    "        trades.sort_index()\n",
    "        trades_range = trades.loc[(trades.index >= start_date) & (trades.index <= end_date)]\n",
    "    except OSError as error_msg:\n",
    "        pass\n",
    "    except Exception as error_msg:\n",
    "        pass\n",
    "\n",
    "    return error_msg, trades_range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_trades_db_integrity_ok (trades):\n",
    "    # Verify that all the values have the correct signs. When buying quantity is positive and amount is negative, when selling quantity is negative and amount is positive, price is always positive, taxes and fees are always negative\n",
    "    # Returns True if everything ok or False if something is wrong. Returns error message\n",
    "\n",
    "    error_msg = ''\n",
    "    if not trades.loc[(trades['Tr_BuySell']=='B') & (trades['Tr_Quantity']<0)].empty:\n",
    "        error_msg = 'There are Buys whith negative quantity. '\n",
    "\n",
    "    if not trades.loc[(trades['Tr_BuySell']=='B') & ((trades['Tr_GrossAmount']>0) | (trades['Tr_NetAmount']>0))].empty:\n",
    "        error_msg = error_msg + 'There are Buys with positive Amount. '\n",
    "\n",
    "    if not trades.loc[(trades['Tr_BuySell']=='S') & (trades['Tr_Quantity']>0)].empty:\n",
    "        error_msg = error_msg + 'There are Sells with positive Quantity. '\n",
    "\n",
    "    if not trades.loc[(trades['Tr_BuySell']=='S') & ((trades['Tr_GrossAmount']<0) | (trades['Tr_NetAmount']<0))].empty:\n",
    "        error_msg = error_msg + 'There are Sells with negative Amount. '\n",
    "    \n",
    "    if not trades.loc[(trades['Tr_Price']<0) | (trades['Tr_NetPrice']<0)].empty:\n",
    "        error_msg = error_msg + 'There are negative Prices. '\n",
    "\n",
    "    if not trades.loc[(trades['Tr_Fees']>0) | (trades['Tr_Taxes']>0)].empty:\n",
    "        error_msg = error_msg + 'There are Fees or Taxes with positive values. '\n",
    "\n",
    "    if not trades.loc[(trades['Tr_BuySell']!='B') & (trades['Tr_BuySell']!='S') & (trades['Tr_BuySell']!='SPLIT')].empty:\n",
    "        error_msg = error_msg + 'There are trades that are neither Buys nor Sells nor Splits. '\n",
    "\n",
    "    if error_msg != '':\n",
    "        return False, error_msg\n",
    "    else:\n",
    "        return True, ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_trades_by_date_and_calculate_pnl_of_daytrades(trades):\n",
    "    # Returns the trades grouped by date, client, book, strategy and asset, assuring there is one trade per date, and calculates the pnl of eventual daytrades\n",
    "\n",
    "    # Necessary to group the records to assure the index key is unique\n",
    "    groupby_list = ['TradeDate','Client','Book','Strategy','Asset','Tr_SettlDate']\n",
    "    columns_tosum = ['Tr_Quantity','Tr_Price','Tr_Fees','Tr_Taxes','Tr_GrossAmount','Tr_NetAmount']\n",
    "    buys = trades.loc[trades['Tr_BuySell']=='B'].groupby(by=groupby_list,)[columns_tosum].sum().copy()\n",
    "    buys['Tr_Price'] = -buys['Tr_GrossAmount']/buys['Tr_Quantity']\n",
    "    buys['Tr_NetPrice'] = -buys['Tr_NetAmount']/buys['Tr_Quantity']\n",
    "    sells = trades.loc[trades['Tr_BuySell']=='S'].groupby(by=groupby_list,)[columns_tosum].sum().copy()\n",
    "    sells['Tr_Price'] = -sells['Tr_GrossAmount']/sells['Tr_Quantity']\n",
    "    sells['Tr_NetPrice'] = -sells['Tr_NetAmount']/sells['Tr_Quantity']\n",
    "\n",
    "    # Carve out the daytrades to calculate the respective PnL and adjust the quantity bought or sold, after processing the daytrade\n",
    "    daytrades = pd.merge(buys, sells, how='inner', on=groupby_list, suffixes=('_buy','_sell'))\n",
    "    daytrades['Tr_Realized_Pnl'] = 0.00\n",
    "    buys.drop(labels=daytrades.index, axis=0, inplace=True)\n",
    "    sells.drop(labels=daytrades.index, axis=0, inplace=True)\n",
    "\n",
    "    # Add columns BuySell and Realized_Pnl to buys and sells\n",
    "    buys['Tr_BuySell'] = 'B'\n",
    "    buys['Tr_Realized_Pnl'] = 0.00\n",
    "    sells['Tr_BuySell'] = 'S'\n",
    "    sells['Tr_Realized_Pnl'] = 0.00\n",
    "\n",
    "    # Select the subset where quantity bought >= the quantity sold, calculates the Realized_Pnl of the daytrades, adjusts the quantity and relative amounts bought and drops the sells (that will be = 0)\n",
    "    # When Quantity_buy = Quantity_sell, all quantities and amounts end up = 0 and only the Realized Pnl can be != 0. We have to keep the respective row to store the Realized_Pnl\n",
    "\n",
    "    buy_gt_sell = daytrades.loc[daytrades['Tr_Quantity_buy'] >= abs(daytrades['Tr_Quantity_sell'])].copy()\n",
    "\n",
    "    if not buy_gt_sell.empty:\n",
    "        buy_gt_sell['Tr_Realized_Pnl'] = round(buy_gt_sell['Tr_NetAmount_sell'] + (buy_gt_sell['Tr_Quantity_sell'] * buy_gt_sell['Tr_NetPrice_buy']), 2)\n",
    "        buy_gt_sell['Tr_Fees_buy'] = round(buy_gt_sell['Tr_Fees_buy'] * (1 + (buy_gt_sell['Tr_Quantity_sell'] / buy_gt_sell['Tr_Quantity_buy'])), 2)\n",
    "        buy_gt_sell['Tr_Taxes_buy'] = round(buy_gt_sell['Tr_Taxes_buy'] * (1 + (buy_gt_sell['Tr_Quantity_sell'] / buy_gt_sell['Tr_Quantity_buy'])), 2)\n",
    "        buy_gt_sell['Tr_Quantity_buy'] = round(buy_gt_sell['Tr_Quantity_buy'] + buy_gt_sell['Tr_Quantity_sell'], 8)\n",
    "        buy_gt_sell['Tr_GrossAmount_buy'] = -round(buy_gt_sell['Tr_Quantity_buy'] * buy_gt_sell['Tr_Price_buy'], 2)\n",
    "        buy_gt_sell['Tr_NetAmount_buy'] = -round(buy_gt_sell['Tr_Quantity_buy'] * buy_gt_sell['Tr_NetPrice_buy'], 2)\n",
    "\n",
    "    buy_gt_sell.drop(['Tr_Quantity_sell','Tr_Price_sell','Tr_Fees_sell','Tr_Taxes_sell','Tr_GrossAmount_sell','Tr_NetAmount_sell','Tr_NetPrice_sell'],axis=1, inplace=True)\n",
    "    buy_gt_sell.rename(columns={'Tr_Quantity_buy':'Tr_Quantity','Tr_Price_buy':'Tr_Price','Tr_Fees_buy':'Tr_Fees','Tr_Taxes_buy':'Tr_Taxes','Tr_GrossAmount_buy':'Tr_GrossAmount','Tr_NetAmount_buy':'Tr_NetAmount','Tr_NetPrice_buy':'Tr_NetPrice'}, inplace=True)\n",
    "    buy_gt_sell['Tr_BuySell'] = 'B'\n",
    "\n",
    "    # Select the subset where Quantity_sell > Quantity_buy, calculates the Realized_Pnl of the daytrades, adjusts the quantity and relative amounts sold and drops the the buys (that will be = 0)\n",
    "    sell_gt_buy = daytrades.loc[daytrades['Tr_Quantity_buy'] < abs(daytrades['Tr_Quantity_sell'])].copy()\n",
    "\n",
    "    if not sell_gt_buy.empty:\n",
    "        sell_gt_buy['Tr_Realized_Pnl'] = round(sell_gt_buy['Tr_NetAmount_buy'] + (sell_gt_buy['Tr_Quantity_buy'] * sell_gt_buy['Tr_NetPrice_sell']), 2)\n",
    "        sell_gt_buy['Tr_Fees_sell'] = round(sell_gt_buy['Tr_Fees_sell'] * (1 + (sell_gt_buy['Tr_Quantity_buy'] / sell_gt_buy['Tr_Quantity_sell'])), 2)\n",
    "        sell_gt_buy['Tr_Taxes_sell'] = round(sell_gt_buy['Tr_Taxes_sell'] * (1 + (sell_gt_buy['Tr_Quantity_buy'] / sell_gt_buy['Tr_Quantity_sell'])), 2)\n",
    "        sell_gt_buy['Tr_Quantity_sell'] = round(sell_gt_buy['Tr_Quantity_sell'] + sell_gt_buy['Tr_Quantity_buy'], 8)\n",
    "        sell_gt_buy['Tr_GrossAmount_sell'] = -round(sell_gt_buy['Tr_Quantity_sell'] * sell_gt_buy['Tr_Price_sell'], 2)\n",
    "        sell_gt_buy['Tr_NetAmount_sell'] = -round(sell_gt_buy['Tr_Quantity_sell'] * sell_gt_buy['Tr_NetPrice_sell'], 2)\n",
    "\n",
    "    sell_gt_buy.drop(['Tr_Quantity_buy','Tr_Price_buy','Tr_Fees_buy','Tr_Taxes_buy','Tr_GrossAmount_buy','Tr_NetAmount_buy','Tr_NetPrice_buy'],axis=1, inplace=True)\n",
    "    sell_gt_buy.rename(columns={'Tr_Quantity_sell':'Tr_Quantity','Tr_Price_sell':'Tr_Price','Tr_Fees_sell':'Tr_Fees','Tr_Taxes_sell':'Tr_Taxes','Tr_GrossAmount_sell':'Tr_GrossAmount','Tr_NetAmount_sell':'Tr_NetAmount','Tr_NetPrice_sell':'Tr_NetPrice'}, inplace=True)\n",
    "    sell_gt_buy['Tr_BuySell'] = 'S'\n",
    "\n",
    "    # Join back buys and sells into the trades_funds dataframe\n",
    "    #trades_grouped = trades.iloc[0:0]\n",
    "    #trades_grouped.set_index(keys=['Client','Book','Strategy','Asset','Tr_SettlDate'], append=True, inplace=True)\n",
    "    trades_grouped = pd.concat([buys, buy_gt_sell, sells, sell_gt_buy])\n",
    "    trades_grouped.reset_index(level='Tr_SettlDate', inplace=True)\n",
    "    trades_grouped.sort_index(axis=0,inplace=True)\n",
    "\n",
    "    return trades_grouped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trades_grouped_by_date(db_path, start_date = pd.to_datetime('1900-01-01'), end_date = pd.to_datetime('2100-12-31')):\n",
    "    # Uploads the trades of the period, group the trades by date, client, book, strategy and asset, assuring there is one trade per date, and calculates the pnl of eventual daytrades\n",
    "    # if no dates are passed, uses all the trades available\n",
    "    \n",
    "    error_msg, trades = load_trades(db_path, start_date, end_date)\n",
    "    if error_msg != '':\n",
    "        print(error_msg)\n",
    "        quit\n",
    "\n",
    "    integrity_ok, error_msg = is_trades_db_integrity_ok(trades)\n",
    "    if not integrity_ok:\n",
    "        print(error_msg)\n",
    "        quit\n",
    "\n",
    "    trades_grouped = group_trades_by_date_and_calculate_pnl_of_daytrades(trades)\n",
    "\n",
    "    return trades_grouped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_portfolio(trades, db_path_portf):\n",
    "    # This function updates the portfolio with trades\n",
    "    # Uploads the portfolio db\n",
    "    portfolio = pd.read_csv(db_path_portf, delimiter=';', \n",
    "                              dtype={'Client': int, 'Book': str, 'Strategy': str, 'Asset': str, 'TradeDate': str, 'Tr_BuySell':str, 'Tr_Quantity':float, 'Tr_Price': float, 'Tr_Fees': float, 'Tr_Taxes':float, 'Tr_GrossAmount': float, 'Tr_NetAmount': float, 'Tr_NetPrice': float, 'Tr_SettlDate': str, 'Tr_BuyDate': str}, \n",
    "                              index_col=['TradeDate'])\n",
    "    portfolio.index = pd.to_datetime(portfolio.index,format='%Y-%m-%d')\n",
    "\n",
    "    # Drops all rows where TradeDate >= first TradeDate from Trades. We aasume that the portfolio is always recalculated with the trades, i.e., trades df must have all the trades with TradeDate >= first date\n",
    "    trades.reset_index(inplace=True)\n",
    "    trades.set_index(keys=['TradeDate'], append=False, inplace=True)\n",
    "    portfolio = portfolio.loc[(portfolio.index < trades.first_valid_index())]\n",
    "    \n",
    "    # Adds the trades to the portfolio\n",
    "    portfolio = pd.concat([portfolio, trades])\n",
    "    portfolio.reset_index(inplace=True)\n",
    "    portfolio.set_index(keys=['Client','Book','Strategy','Asset','TradeDate'], append=False, inplace=True)\n",
    "    portfolio.sort_index(inplace=True)\n",
    "    portfolio['Quantity'] = portfolio['Tr_Quantity']\n",
    "    portfolio_grouped = portfolio.groupby(['Client','Book','Strategy','Asset'])\n",
    "    portfolio['Quantity'] = portfolio_grouped['Tr_Quantity'].cumsum()\n",
    "    portfolio['Quantity_prev'] = portfolio_grouped['Quantity'].shift(1)\n",
    "    portfolio['Quantity_prev'].fillna(value=0,inplace=True)\n",
    "    portfolio['Price'].fillna(value=0,inplace=True)\n",
    "    portfolio['Value'].fillna(value=0,inplace=True)\n",
    "    portfolio['Cost'].fillna(value=0,inplace=True)\n",
    "    portfolio['Realized_Pnl'].fillna(value=0,inplace=True)\n",
    "\n",
    "    # Calculates the Cost and Realized_Pnl of the portfolio\n",
    "    for group_name, group_data in portfolio_grouped:\n",
    "        cost_dm1 = 0\n",
    "        realized_pnl_dm1 = 0\n",
    "\n",
    "        for idx in group_data.index:\n",
    "\n",
    "            if portfolio.loc[idx,'Quantity_prev'] == 0:\n",
    "                cost = portfolio.loc[idx,'Tr_NetPrice']\n",
    "                realized_pnl = realized_pnl_dm1 + portfolio.loc[idx,'Tr_Realized_Pnl']\n",
    "            elif (portfolio.loc[idx,'Quantity_prev'] > 0 and portfolio.loc[idx,'Tr_Quantity'] > 0) or (portfolio.loc[idx,'Quantity_prev'] < 0 and portfolio.loc[idx,'Tr_Quantity'] < 0):\n",
    "                cost = round(((cost_dm1 * portfolio.loc[idx,'Quantity_prev']) - portfolio.loc[idx,'Tr_NetAmount']) / portfolio.loc[idx,'Quantity'], 8)\n",
    "                realized_pnl = realized_pnl_dm1 + portfolio.loc[idx,'Tr_Realized_Pnl']\n",
    "            else:\n",
    "                if abs(portfolio.loc[idx,'Quantity_prev']) > abs(portfolio.loc[idx,'Tr_Quantity']):\n",
    "                    cost = cost_dm1\n",
    "                    realized_pnl = round(realized_pnl_dm1  + portfolio.loc[idx,'Tr_Realized_Pnl'] + (portfolio.loc[idx,'Tr_NetAmount'] + (cost_dm1 * portfolio.loc[idx,'Tr_Quantity'])), 2)\n",
    "                elif abs(portfolio.loc[idx,'Quantity_prev']) < abs(portfolio.loc[idx,'Tr_Quantity']):\n",
    "                    cost = portfolio.loc[idx,'Tr_NetPrice']\n",
    "                    realized_pnl = round(realized_pnl_dm1  + portfolio.loc[idx,'Tr_Realized_Pnl'] + ((portfolio.loc[idx,'Tr_NetPrice'] - cost_dm1) * portfolio.loc[idx,'Quantity_prev']), 2)\n",
    "                else:\n",
    "                    cost = 0\n",
    "                    realized_pnl = round(realized_pnl_dm1  + portfolio.loc[idx,'Tr_Realized_Pnl'] + (portfolio.loc[idx,'Tr_NetAmount'] + (cost_dm1 * portfolio.loc[idx,'Tr_Quantity'])), 2)\n",
    "\n",
    "            portfolio.loc[idx,'Cost'] = cost\n",
    "            portfolio.loc[idx,'Realized_Pnl'] = realized_pnl\n",
    "            cost_dm1 = cost\n",
    "            realized_pnl_dm1 = realized_pnl\n",
    "\n",
    "    # Saves the updated portfolio to the database. We are not saving previous versions of the portfolio as we do with the trades. Should we save them?\n",
    "    portfolio.to_csv(db_path_portf, sep=';',header=['Tr_SettlDate', 'Tr_Quantity', 'Tr_Price', 'Tr_Fees', 'Tr_Taxes', 'Tr_GrossAmount', 'Tr_NetAmount', 'Tr_NetPrice', 'Tr_BuySell', 'Tr_Realized_Pnl', 'Quantity', 'Quantity_prev', 'Price', 'Value', 'Cost', 'Realized_Pnl'], \n",
    "                        index_label=['Client', 'Book', 'Strategy', 'Asset', 'TradeDate'])\n",
    "\n",
    "    return portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cash_flows(db_path, trades):\n",
    "    # Sum the cash flows to the trades['Realized_Pnl']\n",
    "    \n",
    "    # Uploads the cashflow database\n",
    "    cx_flows = pd.read_csv('D:\\Investiments\\Databases\\Portfolios\\client-000001\\CxFlows.csv', delimiter=';', \n",
    "                            dtype={'TradeDate': str,'Client': int,'Book': str,'Strategy': str,'RiskClass': str,'AssetClass': str,'CxFlowType': str,'Asset':str ,'Currency':str,'CrDb': str,'CxFlowValue': float}) \n",
    "    cx_flows['TradeDate'] = pd.to_datetime(cx_flows['TradeDate'],format='%Y-%m-%d')\n",
    "\n",
    "    # Assures that there is one date per Client, Book, Strategy and Asset\n",
    "    groupby_list = ['Client','Book','Strategy','Asset','TradeDate']\n",
    "    columns_tosum = ['CxFlowValue']\n",
    "    cx_flows_grouped = cx_flows.groupby(by=groupby_list,)[columns_tosum].sum().copy()\n",
    "    \n",
    "    # Puts the index in the right order\n",
    "    trades.reset_index(inplace=True)\n",
    "    trades.set_index(keys=['Client','Book','Strategy','Asset','TradeDate'], append=False, inplace=True)\n",
    "\n",
    "    # Performs an outerjoin to assure all the lines in both dataframes are used, replace the eventual NaN by 0 and NaT by ''\n",
    "    trades = pd.concat([trades,cx_flows_grouped])\n",
    "    trades.fillna(value=0,inplace=True)\n",
    "    trades.loc[trades['Tr_SettlDate']==0,'Tr_SettlDate'] = ''\n",
    "    trades.loc[trades['Tr_BuySell']==0,'Tr_BuySell'] = ''\n",
    "    \n",
    "    # Sums the cash flows to the Tr_Realized_Pnl and drops the CxFlowValue column that is not necessary\n",
    "    trades['Tr_Realized_Pnl'] += trades['CxFlowValue']\n",
    "    trades.drop('CxFlowValue',axis=1, inplace=True)\n",
    "    \n",
    "    return trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploads the trades of the period, group the trades by date, client, book, strategy and asset, assuring there is one trade per date, and calculates the pnl of eventual daytrades\n",
    "db_path_portf = 'D:\\Investiments\\Databases\\Portfolios\\client-000001\\Portfolio.csv'\n",
    "start_date = pd.to_datetime('2015-08-24')\n",
    "end_date = pd.to_datetime('2022-06-28')\n",
    "all_trades = []\n",
    "\n",
    "# Get Funds trades\n",
    "db_path_trades = 'D:\\Investiments\\Databases\\Portfolios\\client-000001\\Trades-Funds.csv'\n",
    "trades = get_trades_grouped_by_date(db_path_trades, start_date, end_date)\n",
    "all_trades = trades\n",
    "\n",
    "# Get Stocks trades\n",
    "db_path_trades = 'D:\\Investiments\\Databases\\Portfolios\\client-000001\\Trades-Equities.csv'\n",
    "trades = get_trades_grouped_by_date(db_path_trades, start_date, end_date)\n",
    "all_trades = pd.concat([all_trades, trades])\n",
    "\n",
    "# Add cash flows to Realized_Pnl\n",
    "db_pah_trades = 'D:\\Investiments\\Databases\\Portfolios\\client-000001\\CxFlows.csv'\n",
    "all_trades = get_cash_flows(db_path_trades, all_trades)\n",
    "\n",
    "# Updates the portfolio with the trades of all assets in the specified range os dates\n",
    "portfolio = update_portfolio(all_trades, db_path_portf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio.last_valid_index()[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio.multiindex.get_level_values(4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('Venv1': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "578b54580852c8f3f942ce9823d34fceda592a2ee81225501e430f36dd9de2ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
